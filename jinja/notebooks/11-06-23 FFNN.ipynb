{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148a9670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 15:09:56.503731: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 15:09:56.567067: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-16 15:09:56.569087: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-16 15:09:56.569097: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-16 15:09:56.897272: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-16 15:09:56.897300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-16 15:09:56.897303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from math import sqrt\n",
    "sys.path.append('../..')\n",
    "from modules import utils\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62b979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afe661",
   "metadata": {},
   "source": [
    "#### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>city</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>pm2_5_calibrated_value</th>\n",
       "      <th>pm2_5_raw_value</th>\n",
       "      <th>pm10_raw_value</th>\n",
       "      <th>pm10_calibrated_value</th>\n",
       "      <th>site_id</th>\n",
       "      <th>device_number</th>\n",
       "      <th>device_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jinja Main Street, Jinja</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>Jinja</td>\n",
       "      <td>2021-09-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60d058c8048305120d2d6142</td>\n",
       "      <td>689753</td>\n",
       "      <td>aq_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jinja Main Street, Jinja</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>Jinja</td>\n",
       "      <td>2021-09-01 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60d058c8048305120d2d6142</td>\n",
       "      <td>689753</td>\n",
       "      <td>aq_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jinja Main Street, Jinja</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>Jinja</td>\n",
       "      <td>2021-09-01 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60d058c8048305120d2d6142</td>\n",
       "      <td>689753</td>\n",
       "      <td>aq_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jinja Main Street, Jinja</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>Jinja</td>\n",
       "      <td>2021-09-01 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60d058c8048305120d2d6142</td>\n",
       "      <td>689753</td>\n",
       "      <td>aq_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jinja Main Street, Jinja</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>Jinja</td>\n",
       "      <td>2021-09-01 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60d058c8048305120d2d6142</td>\n",
       "      <td>689753</td>\n",
       "      <td>aq_23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site_name  latitude  longitude   city  \\\n",
       "0  Jinja Main Street, Jinja  0.437337  33.211051  Jinja   \n",
       "1  Jinja Main Street, Jinja  0.437337  33.211051  Jinja   \n",
       "2  Jinja Main Street, Jinja  0.437337  33.211051  Jinja   \n",
       "3  Jinja Main Street, Jinja  0.437337  33.211051  Jinja   \n",
       "4  Jinja Main Street, Jinja  0.437337  33.211051  Jinja   \n",
       "\n",
       "                  timestamp  pm2_5_calibrated_value  pm2_5_raw_value  \\\n",
       "0 2021-09-01 00:00:00+00:00                     NaN              NaN   \n",
       "1 2021-09-01 01:00:00+00:00                     NaN              NaN   \n",
       "2 2021-09-01 02:00:00+00:00                     NaN              NaN   \n",
       "3 2021-09-01 03:00:00+00:00                     NaN              NaN   \n",
       "4 2021-09-01 04:00:00+00:00                     NaN              NaN   \n",
       "\n",
       "   pm10_raw_value  pm10_calibrated_value                   site_id  \\\n",
       "0             NaN                    NaN  60d058c8048305120d2d6142   \n",
       "1             NaN                    NaN  60d058c8048305120d2d6142   \n",
       "2             NaN                    NaN  60d058c8048305120d2d6142   \n",
       "3             NaN                    NaN  60d058c8048305120d2d6142   \n",
       "4             NaN                    NaN  60d058c8048305120d2d6142   \n",
       "\n",
       "   device_number device_name  \n",
       "0         689753       aq_23  \n",
       "1         689753       aq_23  \n",
       "2         689753       aq_23  \n",
       "3         689753       aq_23  \n",
       "4         689753       aq_23  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jinja_df = pd.read_csv('../data/jinja_data.csv', parse_dates=['timestamp'])\n",
    "jinja_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8a2954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitudes = jinja_df['latitude'].unique()\n",
    "longitudes = jinja_df['longitude'].unique()\n",
    "device_ids = jinja_df['device_number'].unique()\n",
    "len(latitudes), len(longitudes), len(device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07dea145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>pm2_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>452909.0</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>12.2844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>452910.0</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>11.6507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>452911.0</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>22.3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>452912.0</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>17.4937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>452913.0</td>\n",
       "      <td>0.437337</td>\n",
       "      <td>33.211051</td>\n",
       "      <td>25.1622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  latitude  longitude    pm2_5\n",
       "0  452909.0  0.437337  33.211051  12.2844\n",
       "1  452910.0  0.437337  33.211051  11.6507\n",
       "2  452911.0  0.437337  33.211051  22.3980\n",
       "3  452912.0  0.437337  33.211051  17.4937\n",
       "4  452913.0  0.437337  33.211051  25.1622"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame()\n",
    "cols = ['timestamp', 'latitude', 'longitude', 'pm2_5_calibrated_value']\n",
    "for i, device_id in enumerate(device_ids):\n",
    "    device_df = utils.get_device_data(jinja_df, device_id, cols)\n",
    "    processed_df = utils.preprocessing(device_df)\n",
    "    final_df = pd.concat([final_df, processed_df])\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593969e",
   "metadata": {},
   "source": [
    "#### Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbae94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffnn(X_train, y_train, epochs=1000, optimizer='RMSProp', dropout=0.2):\n",
    "    model = tf.keras.Sequential() \n",
    "    model.add(Input(shape=(X_train.shape[1],), name='Input-Layer')) \n",
    "    model.add(Dropout(rate=dropout))\n",
    "    model.add(Dense(128, activation='relu', name='Hidden-Layer1'))\n",
    "    model.add(Dropout(rate=dropout))\n",
    "    model.add(Dense(32, activation='relu', name='Hidden-Layer2'))\n",
    "    model.add(Dropout(rate=dropout))\n",
    "    model.add(Dense(1, activation='linear', name='Output-Layer')) \n",
    "\n",
    "    model.compile(optimizer=optimizer, # default='rmsprop', an algorithm to be used in backpropagation\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 )\n",
    "    checkpoint = ModelCheckpoint('../models/ffnn_checkpoint.h5', monitor='val_loss', save_best_only=True,\n",
    "                                save_weights_only=False)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=300)\n",
    "    model.fit(X_train, \n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=epochs, # default=1, Number of epochs to train the model\n",
    "          callbacks=[checkpoint, early_stopping],\n",
    "          validation_split=0.2, \n",
    "         )\n",
    "    best_model = keras.models.load_model('../models/ffnn_checkpoint.h5')\n",
    "    return best_model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484a957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_data(X):\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_scaled = X.copy()\n",
    "#     X_scaled[:, 0] = scaler.fit_transform(X[:, 0].reshape(-1, 1)).flatten()\n",
    "#     return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2795c0b",
   "metadata": {},
   "source": [
    "#### delete from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# device_indices = final_df[final_df.latitude==latitudes[idx]].index\n",
    "# device_df = jinja_df[jinja_df.device_number == device_ids[idx]]\n",
    "# assert(len(device_indices) == len(device_df)-device_df.pm2_5_calibrated_value.isna().sum())\n",
    "# test_df = final_df.loc[device_indices]\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113bbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.concat([final_df, test_df]).drop_duplicates(keep=False)\n",
    "# assert(len(train_df.longitude.unique()) == len(longitudes)-1)\n",
    "# assert len(final_df) == len(test_df) + len(train_df)\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c4dae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_df.iloc[:, 0:-1]\n",
    "# y_train = train_df.iloc[:, -1]\n",
    "# X_train, y_train = np.array(X_train), np.array(y_train)#.reshape(-1, 1)\n",
    "\n",
    "# X_test = test_df.iloc[:, 0:-1]\n",
    "# y_test = test_df.iloc[:, -1]\n",
    "# X_test, y_test = np.array(X_test), np.array(y_test)#.reshape(-1, 1)\n",
    "# X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b81083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98e539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled = scale_data(X_train)\n",
    "# X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c01784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ffnn(X_train, y_train, 500, 'RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38805ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test) \n",
    "# y_pred.shape, y_test.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46a83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(y_pred), np.mean(y_pred), np.max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0aaefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b5965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred = model.predict(X_train)\n",
    "# mean_squared_error(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03798db5",
   "metadata": {},
   "source": [
    "#### end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d0392e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(final_df, idx):\n",
    "    device_indices = final_df[final_df.latitude==latitudes[idx]].index\n",
    "    device_df = jinja_df[jinja_df.device_number == device_ids[idx]]\n",
    "    assert(len(device_indices) == len(device_df)-device_df.pm2_5_calibrated_value.isna().sum())\n",
    "    \n",
    "    test_df = final_df.loc[device_indices]\n",
    "    assert(len(test_df.longitude.unique()) == 1)\n",
    "    \n",
    "    train_df = pd.concat([final_df, test_df]).drop_duplicates(keep=False)\n",
    "    assert(len(train_df.longitude.unique()) == len(longitudes)-1)\n",
    "    assert len(final_df) == len(test_df) + len(train_df)\n",
    "    \n",
    "    X_train = train_df.iloc[:, 0:-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)#.reshape(-1, 1)\n",
    "#     X_train_scaled = scale_data(X_train)\n",
    "    \n",
    "    X_test = test_df.iloc[:, 0:-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)#.reshape(-1, 1)\n",
    "#     X_test_scaled = scale_data(X_test)\n",
    "    \n",
    "    model = ffnn(X_train, y_train)\n",
    "    y_pred = model.predict(X_test) \n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    return rmse, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574c00d",
   "metadata": {},
   "source": [
    "#### delete from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fcf4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "device_indices = final_df[final_df.latitude==latitudes[idx]].index\n",
    "device_df = jinja_df[jinja_df.device_number == device_ids[idx]]\n",
    "test_df = final_df.loc[device_indices]\n",
    "train_df = pd.concat([final_df, test_df]).drop_duplicates(keep=False)\n",
    "X_train = train_df.iloc[:, 0:-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test = test_df.iloc[:, 0:-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4263fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "289/289 [==============================] - 1s 1ms/step - loss: 96044528.0000 - val_loss: 1578.1554\n",
      "Epoch 2/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 32200.2227 - val_loss: 1565.7485\n",
      "Epoch 3/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 8392.7725 - val_loss: 1551.1180\n",
      "Epoch 4/1000\n",
      "289/289 [==============================] - 0s 899us/step - loss: 5714.4663 - val_loss: 1535.6742\n",
      "Epoch 5/1000\n",
      "289/289 [==============================] - 0s 943us/step - loss: 11159.9082 - val_loss: 1522.3210\n",
      "Epoch 6/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 8234.6895 - val_loss: 1507.4973\n",
      "Epoch 7/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3450.0317 - val_loss: 1491.8115\n",
      "Epoch 8/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 997.2740 - val_loss: 1475.4044\n",
      "Epoch 9/1000\n",
      "289/289 [==============================] - 0s 992us/step - loss: 2047.6837 - val_loss: 1459.7737\n",
      "Epoch 10/1000\n",
      "289/289 [==============================] - 0s 997us/step - loss: 1216.2286 - val_loss: 1443.8918\n",
      "Epoch 11/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2666.8003 - val_loss: 1428.8391\n",
      "Epoch 12/1000\n",
      "289/289 [==============================] - 0s 981us/step - loss: 4256.6631 - val_loss: 1414.0846\n",
      "Epoch 13/1000\n",
      "289/289 [==============================] - 0s 960us/step - loss: 3184.5452 - val_loss: 1399.3207\n",
      "Epoch 14/1000\n",
      "289/289 [==============================] - 0s 963us/step - loss: 1535.4371 - val_loss: 1384.1450\n",
      "Epoch 15/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 846.9312 - val_loss: 1368.8240\n",
      "Epoch 16/1000\n",
      "289/289 [==============================] - 0s 866us/step - loss: 1253.1233 - val_loss: 1353.8594\n",
      "Epoch 17/1000\n",
      "289/289 [==============================] - 0s 957us/step - loss: 3058.6353 - val_loss: 1339.8201\n",
      "Epoch 18/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1001.1613 - val_loss: 1325.1230\n",
      "Epoch 19/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1272.1696 - val_loss: 1310.5663\n",
      "Epoch 20/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 716.0217 - val_loss: 1295.9229\n",
      "Epoch 21/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 5556.7065 - val_loss: 1283.1241\n",
      "Epoch 22/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 1099.7916 - val_loss: 1269.1312\n",
      "Epoch 23/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 881.6954 - val_loss: 1255.1177\n",
      "Epoch 24/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 1310.6936 - val_loss: 1241.6610\n",
      "Epoch 25/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 1225.0593 - val_loss: 1228.2527\n",
      "Epoch 26/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 668.1254 - val_loss: 1214.6427\n",
      "Epoch 27/1000\n",
      "289/289 [==============================] - 0s 950us/step - loss: 1255.3512 - val_loss: 1201.3765\n",
      "Epoch 28/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 653.9072 - val_loss: 1188.2723\n",
      "Epoch 29/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 684.8348 - val_loss: 1175.2938\n",
      "Epoch 30/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 716.5308 - val_loss: 1162.3765\n",
      "Epoch 31/1000\n",
      "289/289 [==============================] - 0s 980us/step - loss: 640.9520 - val_loss: 1149.5319\n",
      "Epoch 32/1000\n",
      "289/289 [==============================] - 0s 964us/step - loss: 4516.7866 - val_loss: 1137.8202\n",
      "Epoch 33/1000\n",
      "289/289 [==============================] - 0s 921us/step - loss: 702.1309 - val_loss: 1125.4728\n",
      "Epoch 34/1000\n",
      "289/289 [==============================] - 0s 965us/step - loss: 1894.7970 - val_loss: 1113.6160\n",
      "Epoch 35/1000\n",
      "289/289 [==============================] - 0s 995us/step - loss: 1453.4479 - val_loss: 1101.8684\n",
      "Epoch 36/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 651.1813 - val_loss: 1089.9265\n",
      "Epoch 37/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 18378.7363 - val_loss: 1079.2131\n",
      "Epoch 38/1000\n",
      "289/289 [==============================] - 0s 913us/step - loss: 826.4730 - val_loss: 1068.7771\n",
      "Epoch 39/1000\n",
      "289/289 [==============================] - 0s 955us/step - loss: 9826.2998 - val_loss: 1058.7413\n",
      "Epoch 40/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1253.8099 - val_loss: 1047.8940\n",
      "Epoch 41/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1809.1373 - val_loss: 1037.3109\n",
      "Epoch 42/1000\n",
      "289/289 [==============================] - 0s 995us/step - loss: 2592.6113 - val_loss: 1027.2104\n",
      "Epoch 43/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1260.6104 - val_loss: 1016.7307\n",
      "Epoch 44/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2234.7476 - val_loss: 1006.3276\n",
      "Epoch 45/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4999.1724 - val_loss: 997.3793\n",
      "Epoch 46/1000\n",
      "289/289 [==============================] - 0s 935us/step - loss: 19721.0352 - val_loss: 988.4988\n",
      "Epoch 47/1000\n",
      "289/289 [==============================] - 0s 946us/step - loss: 875.0551 - val_loss: 978.5399\n",
      "Epoch 48/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3658.1995 - val_loss: 969.5745\n",
      "Epoch 49/1000\n",
      "289/289 [==============================] - 0s 971us/step - loss: 6344.6177 - val_loss: 960.9113\n",
      "Epoch 50/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4279.9360 - val_loss: 952.2534\n",
      "Epoch 51/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 6742.5952 - val_loss: 944.4874\n",
      "Epoch 52/1000\n",
      "289/289 [==============================] - 0s 907us/step - loss: 6366.2793 - val_loss: 936.1373\n",
      "Epoch 53/1000\n",
      "289/289 [==============================] - 0s 930us/step - loss: 11306.5020 - val_loss: 928.2352\n",
      "Epoch 54/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 1773.2279 - val_loss: 919.7623\n",
      "Epoch 55/1000\n",
      "289/289 [==============================] - 0s 915us/step - loss: 3186.6055 - val_loss: 911.7020\n",
      "Epoch 56/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 4211.9600 - val_loss: 903.9726\n",
      "Epoch 57/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 3013.7148 - val_loss: 896.2308\n",
      "Epoch 58/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1192.2982 - val_loss: 888.2128\n",
      "Epoch 59/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 6572.9302 - val_loss: 881.2800\n",
      "Epoch 60/1000\n",
      "289/289 [==============================] - 0s 856us/step - loss: 1594.3319 - val_loss: 873.5620\n",
      "Epoch 61/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4565.1548 - val_loss: 865.9891\n",
      "Epoch 62/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2874.5154 - val_loss: 859.7502\n",
      "Epoch 63/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1063.2576 - val_loss: 852.5652\n",
      "Epoch 64/1000\n",
      "289/289 [==============================] - 0s 983us/step - loss: 4503.6089 - val_loss: 846.4916\n",
      "Epoch 65/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 771.2175 - val_loss: 839.1606\n",
      "Epoch 66/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3785.5776 - val_loss: 832.8114\n",
      "Epoch 67/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 859.6304 - val_loss: 826.0008\n",
      "Epoch 68/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 843.1415 - val_loss: 819.4763\n",
      "Epoch 69/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 7906.5381 - val_loss: 813.7282\n",
      "Epoch 70/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1576.8639 - val_loss: 807.6979\n",
      "Epoch 71/1000\n",
      "289/289 [==============================] - 0s 949us/step - loss: 455.5029 - val_loss: 801.1689\n",
      "Epoch 72/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 976.8599 - val_loss: 795.3232\n",
      "Epoch 73/1000\n",
      "289/289 [==============================] - 0s 990us/step - loss: 3555.5764 - val_loss: 790.4120\n",
      "Epoch 74/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 11980.0029 - val_loss: 785.2393\n",
      "Epoch 75/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1021.4307 - val_loss: 779.8032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2167.2141 - val_loss: 774.5467\n",
      "Epoch 77/1000\n",
      "289/289 [==============================] - 0s 971us/step - loss: 9937.6162 - val_loss: 770.6545\n",
      "Epoch 78/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 953.7294 - val_loss: 765.6997\n",
      "Epoch 79/1000\n",
      "289/289 [==============================] - 0s 965us/step - loss: 1629.9756 - val_loss: 761.1424\n",
      "Epoch 80/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3536.6772 - val_loss: 756.7839\n",
      "Epoch 81/1000\n",
      "289/289 [==============================] - 0s 917us/step - loss: 2670.0579 - val_loss: 752.7768\n",
      "Epoch 82/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1188.2843 - val_loss: 748.5137\n",
      "Epoch 83/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1568.5486 - val_loss: 744.4105\n",
      "Epoch 84/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 425.9561 - val_loss: 740.1152\n",
      "Epoch 85/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 662.4094 - val_loss: 736.2399\n",
      "Epoch 86/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 453.0069 - val_loss: 732.4964\n",
      "Epoch 87/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 762.7338 - val_loss: 728.9595\n",
      "Epoch 88/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 422.4614 - val_loss: 725.2563\n",
      "Epoch 89/1000\n",
      "289/289 [==============================] - 0s 896us/step - loss: 419.5550 - val_loss: 721.9005\n",
      "Epoch 90/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 1904.5531 - val_loss: 718.9370\n",
      "Epoch 91/1000\n",
      "289/289 [==============================] - 0s 927us/step - loss: 3629.5139 - val_loss: 716.0605\n",
      "Epoch 92/1000\n",
      "289/289 [==============================] - 0s 951us/step - loss: 424.7169 - val_loss: 713.3966\n",
      "Epoch 93/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4139.5957 - val_loss: 711.0310\n",
      "Epoch 94/1000\n",
      "289/289 [==============================] - 0s 992us/step - loss: 667.1368 - val_loss: 708.7369\n",
      "Epoch 95/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 9297.8604 - val_loss: 706.6785\n",
      "Epoch 96/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 802.7433 - val_loss: 704.2761\n",
      "Epoch 97/1000\n",
      "289/289 [==============================] - 0s 929us/step - loss: 419.5011 - val_loss: 701.9034\n",
      "Epoch 98/1000\n",
      "289/289 [==============================] - 0s 928us/step - loss: 964.1483 - val_loss: 699.7643\n",
      "Epoch 99/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 512.8570 - val_loss: 697.8121\n",
      "Epoch 100/1000\n",
      "289/289 [==============================] - 0s 969us/step - loss: 457.9665 - val_loss: 696.0641\n",
      "Epoch 101/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2429.7742 - val_loss: 694.5125\n",
      "Epoch 102/1000\n",
      "289/289 [==============================] - 0s 953us/step - loss: 581.5577 - val_loss: 692.8632\n",
      "Epoch 103/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3236.9216 - val_loss: 691.4840\n",
      "Epoch 104/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 996.5621 - val_loss: 689.8851\n",
      "Epoch 105/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 991.4144 - val_loss: 688.2769\n",
      "Epoch 106/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 441.9016 - val_loss: 686.8246\n",
      "Epoch 107/1000\n",
      "289/289 [==============================] - 0s 973us/step - loss: 1896.4669 - val_loss: 685.6956\n",
      "Epoch 108/1000\n",
      "289/289 [==============================] - 0s 939us/step - loss: 1508.5752 - val_loss: 684.6113\n",
      "Epoch 109/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1629.0020 - val_loss: 683.2574\n",
      "Epoch 110/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 453.9703 - val_loss: 682.2600\n",
      "Epoch 111/1000\n",
      "289/289 [==============================] - 0s 934us/step - loss: 3229.7415 - val_loss: 681.2794\n",
      "Epoch 112/1000\n",
      "289/289 [==============================] - 0s 989us/step - loss: 510.8271 - val_loss: 680.3076\n",
      "Epoch 113/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2105.9492 - val_loss: 679.3669\n",
      "Epoch 114/1000\n",
      "289/289 [==============================] - 0s 883us/step - loss: 893.2747 - val_loss: 678.3963\n",
      "Epoch 115/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1406.3787 - val_loss: 677.6261\n",
      "Epoch 116/1000\n",
      "289/289 [==============================] - 0s 931us/step - loss: 1703.9943 - val_loss: 676.9095\n",
      "Epoch 117/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1998.4929 - val_loss: 676.2852\n",
      "Epoch 118/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 1184.0731 - val_loss: 675.5504\n",
      "Epoch 119/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4290.2183 - val_loss: 674.9331\n",
      "Epoch 120/1000\n",
      "289/289 [==============================] - 0s 898us/step - loss: 889.0941 - val_loss: 674.5013\n",
      "Epoch 121/1000\n",
      "289/289 [==============================] - 0s 959us/step - loss: 1333.7600 - val_loss: 674.0487\n",
      "Epoch 122/1000\n",
      "289/289 [==============================] - 0s 969us/step - loss: 2418.0869 - val_loss: 673.5107\n",
      "Epoch 123/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 571.7704 - val_loss: 673.0356\n",
      "Epoch 124/1000\n",
      "289/289 [==============================] - 0s 854us/step - loss: 1205.8281 - val_loss: 672.6801\n",
      "Epoch 125/1000\n",
      "289/289 [==============================] - 0s 981us/step - loss: 436.9881 - val_loss: 672.4216\n",
      "Epoch 126/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 466.8650 - val_loss: 672.0513\n",
      "Epoch 127/1000\n",
      "289/289 [==============================] - 0s 965us/step - loss: 2476.0977 - val_loss: 671.7804\n",
      "Epoch 128/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1642.4779 - val_loss: 671.4425\n",
      "Epoch 129/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 716.0441 - val_loss: 671.1629\n",
      "Epoch 130/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 423.0874 - val_loss: 670.9870\n",
      "Epoch 131/1000\n",
      "289/289 [==============================] - 0s 968us/step - loss: 429.3549 - val_loss: 670.8343\n",
      "Epoch 132/1000\n",
      "289/289 [==============================] - 0s 941us/step - loss: 411.7825 - val_loss: 670.7660\n",
      "Epoch 133/1000\n",
      "289/289 [==============================] - 0s 993us/step - loss: 464.8827 - val_loss: 670.6071\n",
      "Epoch 134/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 459.2592 - val_loss: 670.4047\n",
      "Epoch 135/1000\n",
      "289/289 [==============================] - 0s 980us/step - loss: 430.4165 - val_loss: 670.2145\n",
      "Epoch 136/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1128.3461 - val_loss: 670.0723\n",
      "Epoch 137/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 424.1236 - val_loss: 669.9808\n",
      "Epoch 138/1000\n",
      "289/289 [==============================] - 0s 995us/step - loss: 422.1460 - val_loss: 669.8651\n",
      "Epoch 139/1000\n",
      "289/289 [==============================] - 0s 985us/step - loss: 1142.4960 - val_loss: 669.6217\n",
      "Epoch 140/1000\n",
      "289/289 [==============================] - 0s 994us/step - loss: 697.6313 - val_loss: 669.5401\n",
      "Epoch 141/1000\n",
      "289/289 [==============================] - 0s 999us/step - loss: 1118.0175 - val_loss: 669.2767\n",
      "Epoch 142/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 452.6460 - val_loss: 669.0813\n",
      "Epoch 143/1000\n",
      "289/289 [==============================] - 0s 920us/step - loss: 694.0930 - val_loss: 668.8528\n",
      "Epoch 144/1000\n",
      "289/289 [==============================] - 0s 930us/step - loss: 426.3129 - val_loss: 668.7151\n",
      "Epoch 145/1000\n",
      "289/289 [==============================] - 0s 962us/step - loss: 492.4226 - val_loss: 668.5279\n",
      "Epoch 146/1000\n",
      "289/289 [==============================] - 0s 926us/step - loss: 514.8846 - val_loss: 668.2603\n",
      "Epoch 147/1000\n",
      "289/289 [==============================] - 0s 917us/step - loss: 5785.1177 - val_loss: 668.1393\n",
      "Epoch 148/1000\n",
      "289/289 [==============================] - 0s 932us/step - loss: 557.5886 - val_loss: 667.9749\n",
      "Epoch 149/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1656.6174 - val_loss: 667.8306\n",
      "Epoch 150/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 480.0503 - val_loss: 667.7896\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 1ms/step - loss: 421.0814 - val_loss: 667.7689\n",
      "Epoch 152/1000\n",
      "289/289 [==============================] - 0s 939us/step - loss: 430.9088 - val_loss: 667.7184\n",
      "Epoch 153/1000\n",
      "289/289 [==============================] - 0s 962us/step - loss: 553.7941 - val_loss: 667.5731\n",
      "Epoch 154/1000\n",
      "289/289 [==============================] - 0s 990us/step - loss: 1680.0826 - val_loss: 667.3939\n",
      "Epoch 155/1000\n",
      "289/289 [==============================] - 0s 994us/step - loss: 562.3708 - val_loss: 667.3033\n",
      "Epoch 156/1000\n",
      "289/289 [==============================] - 0s 962us/step - loss: 411.6691 - val_loss: 667.3176\n",
      "Epoch 157/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2831.2717 - val_loss: 667.1094\n",
      "Epoch 158/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 578.7726 - val_loss: 667.0937\n",
      "Epoch 159/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 463.4751 - val_loss: 667.1383\n",
      "Epoch 160/1000\n",
      "289/289 [==============================] - 0s 926us/step - loss: 420.2487 - val_loss: 667.2301\n",
      "Epoch 161/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 541.9769 - val_loss: 667.0474\n",
      "Epoch 162/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 472.8265 - val_loss: 667.0489\n",
      "Epoch 163/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3673.6621 - val_loss: 667.0967\n",
      "Epoch 164/1000\n",
      "289/289 [==============================] - 0s 921us/step - loss: 4450.2681 - val_loss: 666.8570\n",
      "Epoch 165/1000\n",
      "289/289 [==============================] - 0s 998us/step - loss: 34589.6289 - val_loss: 666.7598\n",
      "Epoch 166/1000\n",
      "289/289 [==============================] - 0s 915us/step - loss: 424.2216 - val_loss: 666.8026\n",
      "Epoch 167/1000\n",
      "289/289 [==============================] - 0s 909us/step - loss: 2572.4768 - val_loss: 666.8434\n",
      "Epoch 168/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 424.7535 - val_loss: 666.8934\n",
      "Epoch 169/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7123 - val_loss: 666.9658\n",
      "Epoch 170/1000\n",
      "289/289 [==============================] - 0s 953us/step - loss: 418.9399 - val_loss: 666.9570\n",
      "Epoch 171/1000\n",
      "289/289 [==============================] - 0s 911us/step - loss: 514.1475 - val_loss: 667.0046\n",
      "Epoch 172/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 11260.4453 - val_loss: 666.9753\n",
      "Epoch 173/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 7760.0405 - val_loss: 666.8495\n",
      "Epoch 174/1000\n",
      "289/289 [==============================] - 0s 948us/step - loss: 571.7245 - val_loss: 666.8828\n",
      "Epoch 175/1000\n",
      "289/289 [==============================] - 0s 989us/step - loss: 4418.6699 - val_loss: 666.9210\n",
      "Epoch 176/1000\n",
      "289/289 [==============================] - 0s 939us/step - loss: 3831.6836 - val_loss: 666.8737\n",
      "Epoch 177/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3284.9399 - val_loss: 666.6350\n",
      "Epoch 178/1000\n",
      "289/289 [==============================] - 0s 905us/step - loss: 441.2336 - val_loss: 666.7224\n",
      "Epoch 179/1000\n",
      "289/289 [==============================] - 0s 984us/step - loss: 3080.4351 - val_loss: 666.6457\n",
      "Epoch 180/1000\n",
      "289/289 [==============================] - 0s 951us/step - loss: 706.5137 - val_loss: 666.6131\n",
      "Epoch 181/1000\n",
      "289/289 [==============================] - 0s 990us/step - loss: 1672.2366 - val_loss: 666.5150\n",
      "Epoch 182/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1110.0580 - val_loss: 666.5301\n",
      "Epoch 183/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 12496.9033 - val_loss: 666.3546\n",
      "Epoch 184/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3521.0728 - val_loss: 666.3480\n",
      "Epoch 185/1000\n",
      "289/289 [==============================] - 0s 964us/step - loss: 1287.0498 - val_loss: 666.2236\n",
      "Epoch 186/1000\n",
      "289/289 [==============================] - 0s 904us/step - loss: 2079.3438 - val_loss: 666.3488\n",
      "Epoch 187/1000\n",
      "289/289 [==============================] - 0s 898us/step - loss: 37972.4180 - val_loss: 666.2986\n",
      "Epoch 188/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 15922.3936 - val_loss: 666.1418\n",
      "Epoch 189/1000\n",
      "289/289 [==============================] - 0s 918us/step - loss: 429.2662 - val_loss: 666.2211\n",
      "Epoch 190/1000\n",
      "289/289 [==============================] - 0s 899us/step - loss: 2747.7488 - val_loss: 666.0490\n",
      "Epoch 191/1000\n",
      "289/289 [==============================] - 0s 951us/step - loss: 12940.3740 - val_loss: 666.0522\n",
      "Epoch 192/1000\n",
      "289/289 [==============================] - 0s 971us/step - loss: 1913.9130 - val_loss: 666.0070\n",
      "Epoch 193/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 5969.8950 - val_loss: 665.9121\n",
      "Epoch 194/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3596.2385 - val_loss: 665.7379\n",
      "Epoch 195/1000\n",
      "289/289 [==============================] - 0s 952us/step - loss: 2254.0505 - val_loss: 665.6471\n",
      "Epoch 196/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 7977.8965 - val_loss: 665.6658\n",
      "Epoch 197/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1822.8198 - val_loss: 665.6281\n",
      "Epoch 198/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4951.2446 - val_loss: 665.4600\n",
      "Epoch 199/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 12951.2441 - val_loss: 665.4594\n",
      "Epoch 200/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 975.2415 - val_loss: 665.4915\n",
      "Epoch 201/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1673.5137 - val_loss: 665.4196\n",
      "Epoch 202/1000\n",
      "289/289 [==============================] - 0s 963us/step - loss: 7837.2827 - val_loss: 665.6662\n",
      "Epoch 203/1000\n",
      "289/289 [==============================] - 0s 836us/step - loss: 6104.9888 - val_loss: 665.6643\n",
      "Epoch 204/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 501.4908 - val_loss: 665.6150\n",
      "Epoch 205/1000\n",
      "289/289 [==============================] - 0s 990us/step - loss: 1462.6183 - val_loss: 665.7224\n",
      "Epoch 206/1000\n",
      "289/289 [==============================] - 0s 955us/step - loss: 472.5320 - val_loss: 665.7060\n",
      "Epoch 207/1000\n",
      "289/289 [==============================] - 0s 968us/step - loss: 980.1588 - val_loss: 665.7264\n",
      "Epoch 208/1000\n",
      "289/289 [==============================] - 0s 911us/step - loss: 425.4239 - val_loss: 665.8298\n",
      "Epoch 209/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 7266.7544 - val_loss: 665.8762\n",
      "Epoch 210/1000\n",
      "289/289 [==============================] - 0s 976us/step - loss: 1137.0959 - val_loss: 665.9131\n",
      "Epoch 211/1000\n",
      "289/289 [==============================] - 0s 996us/step - loss: 10806.2793 - val_loss: 665.8805\n",
      "Epoch 212/1000\n",
      "289/289 [==============================] - 0s 935us/step - loss: 2713.5676 - val_loss: 665.8780\n",
      "Epoch 213/1000\n",
      "289/289 [==============================] - 0s 945us/step - loss: 10865.0244 - val_loss: 665.7883\n",
      "Epoch 214/1000\n",
      "289/289 [==============================] - 0s 932us/step - loss: 2882.2332 - val_loss: 665.7302\n",
      "Epoch 215/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 4963.6733 - val_loss: 665.6389\n",
      "Epoch 216/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 36501.0977 - val_loss: 665.5374\n",
      "Epoch 217/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 428.6490 - val_loss: 665.6891\n",
      "Epoch 218/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3265.9490 - val_loss: 665.7593\n",
      "Epoch 219/1000\n",
      "289/289 [==============================] - 0s 869us/step - loss: 2099.6477 - val_loss: 665.7814\n",
      "Epoch 220/1000\n",
      "289/289 [==============================] - 0s 979us/step - loss: 453.9609 - val_loss: 665.7838\n",
      "Epoch 221/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 678.0381 - val_loss: 665.8209\n",
      "Epoch 222/1000\n",
      "289/289 [==============================] - 0s 905us/step - loss: 23119.5957 - val_loss: 665.7787\n",
      "Epoch 223/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1972.7944 - val_loss: 665.6620\n",
      "Epoch 224/1000\n",
      "289/289 [==============================] - 0s 911us/step - loss: 3694.5129 - val_loss: 665.7010\n",
      "Epoch 225/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 964us/step - loss: 1696.6873 - val_loss: 665.7922\n",
      "Epoch 226/1000\n",
      "289/289 [==============================] - 0s 937us/step - loss: 643.6166 - val_loss: 665.8380\n",
      "Epoch 227/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 538.6564 - val_loss: 665.8537\n",
      "Epoch 228/1000\n",
      "289/289 [==============================] - 0s 963us/step - loss: 2037.7422 - val_loss: 665.8858\n",
      "Epoch 229/1000\n",
      "289/289 [==============================] - 0s 943us/step - loss: 568.2186 - val_loss: 666.0014\n",
      "Epoch 230/1000\n",
      "289/289 [==============================] - 0s 898us/step - loss: 966.4988 - val_loss: 666.0676\n",
      "Epoch 231/1000\n",
      "289/289 [==============================] - 0s 884us/step - loss: 427.4627 - val_loss: 666.1246\n",
      "Epoch 232/1000\n",
      "289/289 [==============================] - 0s 998us/step - loss: 472.5540 - val_loss: 666.2498\n",
      "Epoch 233/1000\n",
      "289/289 [==============================] - 0s 930us/step - loss: 3666.3237 - val_loss: 666.1536\n",
      "Epoch 234/1000\n",
      "289/289 [==============================] - 0s 940us/step - loss: 5995.1030 - val_loss: 666.1570\n",
      "Epoch 235/1000\n",
      "289/289 [==============================] - 0s 986us/step - loss: 411.7664 - val_loss: 666.3110\n",
      "Epoch 236/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 15211.3848 - val_loss: 666.3260\n",
      "Epoch 237/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3002.0544 - val_loss: 666.3547\n",
      "Epoch 238/1000\n",
      "289/289 [==============================] - 0s 942us/step - loss: 411.9979 - val_loss: 666.5137\n",
      "Epoch 239/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1360.7933 - val_loss: 666.4630\n",
      "Epoch 240/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 411.4201 - val_loss: 666.6061\n",
      "Epoch 241/1000\n",
      "289/289 [==============================] - 0s 953us/step - loss: 3143.1116 - val_loss: 666.5280\n",
      "Epoch 242/1000\n",
      "289/289 [==============================] - 0s 954us/step - loss: 6291.4106 - val_loss: 666.6783\n",
      "Epoch 243/1000\n",
      "289/289 [==============================] - 0s 932us/step - loss: 487.4262 - val_loss: 666.6859\n",
      "Epoch 244/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 426.8139 - val_loss: 666.7793\n",
      "Epoch 245/1000\n",
      "289/289 [==============================] - 0s 935us/step - loss: 412.0338 - val_loss: 666.9001\n",
      "Epoch 246/1000\n",
      "289/289 [==============================] - 0s 926us/step - loss: 421.1933 - val_loss: 666.9227\n",
      "Epoch 247/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2344.9465 - val_loss: 667.0138\n",
      "Epoch 248/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 500.7776 - val_loss: 667.0263\n",
      "Epoch 249/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 413.7793 - val_loss: 667.0212\n",
      "Epoch 250/1000\n",
      "289/289 [==============================] - 0s 990us/step - loss: 7116.1279 - val_loss: 666.9042\n",
      "Epoch 251/1000\n",
      "289/289 [==============================] - 0s 983us/step - loss: 6536.4839 - val_loss: 666.8900\n",
      "Epoch 252/1000\n",
      "289/289 [==============================] - 0s 926us/step - loss: 10875.8545 - val_loss: 667.0126\n",
      "Epoch 253/1000\n",
      "289/289 [==============================] - 0s 865us/step - loss: 464.1949 - val_loss: 667.0651\n",
      "Epoch 254/1000\n",
      "289/289 [==============================] - 0s 883us/step - loss: 2524.6233 - val_loss: 667.1382\n",
      "Epoch 255/1000\n",
      "289/289 [==============================] - 0s 890us/step - loss: 1709.9597 - val_loss: 667.1907\n",
      "Epoch 256/1000\n",
      "289/289 [==============================] - 0s 884us/step - loss: 450.5383 - val_loss: 667.2188\n",
      "Epoch 257/1000\n",
      "289/289 [==============================] - 0s 919us/step - loss: 489.3822 - val_loss: 667.2504\n",
      "Epoch 258/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 450.0639 - val_loss: 667.2795\n",
      "Epoch 259/1000\n",
      "289/289 [==============================] - 0s 945us/step - loss: 1181.4407 - val_loss: 667.4118\n",
      "Epoch 260/1000\n",
      "289/289 [==============================] - 0s 853us/step - loss: 411.7379 - val_loss: 667.5755\n",
      "Epoch 261/1000\n",
      "289/289 [==============================] - 0s 964us/step - loss: 1065.6423 - val_loss: 667.6757\n",
      "Epoch 262/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 481.4703 - val_loss: 667.7661\n",
      "Epoch 263/1000\n",
      "289/289 [==============================] - 0s 963us/step - loss: 411.9196 - val_loss: 667.8696\n",
      "Epoch 264/1000\n",
      "289/289 [==============================] - 0s 917us/step - loss: 412.6728 - val_loss: 667.9418\n",
      "Epoch 265/1000\n",
      "289/289 [==============================] - 0s 992us/step - loss: 411.4128 - val_loss: 668.0458\n",
      "Epoch 266/1000\n",
      "289/289 [==============================] - 0s 969us/step - loss: 1081.9810 - val_loss: 668.1024\n",
      "Epoch 267/1000\n",
      "289/289 [==============================] - 0s 973us/step - loss: 411.9553 - val_loss: 668.1671\n",
      "Epoch 268/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6306 - val_loss: 668.2151\n",
      "Epoch 269/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1228.9503 - val_loss: 668.1249\n",
      "Epoch 270/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5685 - val_loss: 668.1937\n",
      "Epoch 271/1000\n",
      "289/289 [==============================] - 0s 952us/step - loss: 411.7906 - val_loss: 668.2396\n",
      "Epoch 272/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 465.7567 - val_loss: 668.3068\n",
      "Epoch 273/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 3107.6838 - val_loss: 668.2559\n",
      "Epoch 274/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7407 - val_loss: 668.3156\n",
      "Epoch 275/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 412.1455 - val_loss: 668.3635\n",
      "Epoch 276/1000\n",
      "289/289 [==============================] - 0s 957us/step - loss: 1367.9170 - val_loss: 668.3826\n",
      "Epoch 277/1000\n",
      "289/289 [==============================] - 0s 978us/step - loss: 447.9306 - val_loss: 668.4359\n",
      "Epoch 278/1000\n",
      "289/289 [==============================] - 0s 963us/step - loss: 411.7506 - val_loss: 668.4816\n",
      "Epoch 279/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 415.4024 - val_loss: 668.4880\n",
      "Epoch 280/1000\n",
      "289/289 [==============================] - 0s 979us/step - loss: 951.9590 - val_loss: 668.3778\n",
      "Epoch 281/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.4123 - val_loss: 668.5000\n",
      "Epoch 282/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6023 - val_loss: 668.4879\n",
      "Epoch 283/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.3776 - val_loss: 668.4172\n",
      "Epoch 284/1000\n",
      "289/289 [==============================] - 0s 980us/step - loss: 447.9605 - val_loss: 668.3118\n",
      "Epoch 285/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 411.7987 - val_loss: 668.3559\n",
      "Epoch 286/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.9960 - val_loss: 668.4000\n",
      "Epoch 287/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8157 - val_loss: 668.4586\n",
      "Epoch 288/1000\n",
      "289/289 [==============================] - 0s 932us/step - loss: 617.5048 - val_loss: 668.4091\n",
      "Epoch 289/1000\n",
      "289/289 [==============================] - 0s 920us/step - loss: 418.4777 - val_loss: 668.3902\n",
      "Epoch 290/1000\n",
      "289/289 [==============================] - 0s 973us/step - loss: 432.6271 - val_loss: 668.3846\n",
      "Epoch 291/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.9228 - val_loss: 668.4172\n",
      "Epoch 292/1000\n",
      "289/289 [==============================] - 0s 945us/step - loss: 411.5493 - val_loss: 668.5023\n",
      "Epoch 293/1000\n",
      "289/289 [==============================] - 0s 968us/step - loss: 411.6059 - val_loss: 668.5944\n",
      "Epoch 294/1000\n",
      "289/289 [==============================] - 0s 975us/step - loss: 425.5686 - val_loss: 668.5580\n",
      "Epoch 295/1000\n",
      "289/289 [==============================] - 0s 942us/step - loss: 411.8511 - val_loss: 668.5341\n",
      "Epoch 296/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 439.7043 - val_loss: 668.4998\n",
      "Epoch 297/1000\n",
      "289/289 [==============================] - 0s 986us/step - loss: 738.7811 - val_loss: 668.5272\n",
      "Epoch 298/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6860 - val_loss: 668.6216\n",
      "Epoch 299/1000\n",
      "289/289 [==============================] - 0s 986us/step - loss: 448.0727 - val_loss: 668.7258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 432.0098 - val_loss: 668.7585\n",
      "Epoch 301/1000\n",
      "289/289 [==============================] - 0s 920us/step - loss: 411.8080 - val_loss: 668.8379\n",
      "Epoch 302/1000\n",
      "289/289 [==============================] - 0s 851us/step - loss: 501.8224 - val_loss: 668.8775\n",
      "Epoch 303/1000\n",
      "289/289 [==============================] - 0s 940us/step - loss: 411.5946 - val_loss: 668.9645\n",
      "Epoch 304/1000\n",
      "289/289 [==============================] - 0s 1000us/step - loss: 411.8009 - val_loss: 669.0710\n",
      "Epoch 305/1000\n",
      "289/289 [==============================] - 0s 971us/step - loss: 411.5971 - val_loss: 669.0911\n",
      "Epoch 306/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6233 - val_loss: 669.1072\n",
      "Epoch 307/1000\n",
      "289/289 [==============================] - 0s 997us/step - loss: 412.5599 - val_loss: 669.1435\n",
      "Epoch 308/1000\n",
      "289/289 [==============================] - 0s 900us/step - loss: 451.6877 - val_loss: 669.1095\n",
      "Epoch 309/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 415.0574 - val_loss: 669.1654\n",
      "Epoch 310/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6303 - val_loss: 669.1882\n",
      "Epoch 311/1000\n",
      "289/289 [==============================] - 0s 933us/step - loss: 411.5514 - val_loss: 669.1918\n",
      "Epoch 312/1000\n",
      "289/289 [==============================] - 0s 944us/step - loss: 411.6260 - val_loss: 669.2104\n",
      "Epoch 313/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 902.4602 - val_loss: 669.1034\n",
      "Epoch 314/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 533.1999 - val_loss: 669.0546\n",
      "Epoch 315/1000\n",
      "289/289 [==============================] - 0s 912us/step - loss: 411.6953 - val_loss: 669.0594\n",
      "Epoch 316/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8798 - val_loss: 669.0845\n",
      "Epoch 317/1000\n",
      "289/289 [==============================] - 0s 876us/step - loss: 413.2723 - val_loss: 669.1367\n",
      "Epoch 318/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6638 - val_loss: 669.1998\n",
      "Epoch 319/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 415.9913 - val_loss: 669.0880\n",
      "Epoch 320/1000\n",
      "289/289 [==============================] - 0s 899us/step - loss: 502.0707 - val_loss: 668.9552\n",
      "Epoch 321/1000\n",
      "289/289 [==============================] - 0s 986us/step - loss: 1648.3452 - val_loss: 668.8428\n",
      "Epoch 322/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.4897 - val_loss: 668.8915\n",
      "Epoch 323/1000\n",
      "289/289 [==============================] - 0s 824us/step - loss: 411.9271 - val_loss: 668.9230\n",
      "Epoch 324/1000\n",
      "289/289 [==============================] - 0s 862us/step - loss: 411.3958 - val_loss: 668.8920\n",
      "Epoch 325/1000\n",
      "289/289 [==============================] - 0s 964us/step - loss: 426.0762 - val_loss: 668.8644\n",
      "Epoch 326/1000\n",
      "289/289 [==============================] - 0s 893us/step - loss: 422.4332 - val_loss: 668.9271\n",
      "Epoch 327/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 418.6546 - val_loss: 668.8980\n",
      "Epoch 328/1000\n",
      "289/289 [==============================] - 0s 938us/step - loss: 411.9881 - val_loss: 668.8356\n",
      "Epoch 329/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 442.2509 - val_loss: 668.7388\n",
      "Epoch 330/1000\n",
      "289/289 [==============================] - 0s 937us/step - loss: 422.6631 - val_loss: 668.6677\n",
      "Epoch 331/1000\n",
      "289/289 [==============================] - 0s 959us/step - loss: 411.8308 - val_loss: 668.6315\n",
      "Epoch 332/1000\n",
      "289/289 [==============================] - 0s 894us/step - loss: 411.5493 - val_loss: 668.6585\n",
      "Epoch 333/1000\n",
      "289/289 [==============================] - 0s 947us/step - loss: 411.6763 - val_loss: 668.6826\n",
      "Epoch 334/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 452.1678 - val_loss: 668.6271\n",
      "Epoch 335/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 421.6664 - val_loss: 668.5837\n",
      "Epoch 336/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 411.7357 - val_loss: 668.6216\n",
      "Epoch 337/1000\n",
      "289/289 [==============================] - 0s 955us/step - loss: 411.7067 - val_loss: 668.6906\n",
      "Epoch 338/1000\n",
      "289/289 [==============================] - 0s 993us/step - loss: 462.3796 - val_loss: 668.6420\n",
      "Epoch 339/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 418.2468 - val_loss: 668.7145\n",
      "Epoch 340/1000\n",
      "289/289 [==============================] - 0s 935us/step - loss: 636.4119 - val_loss: 668.6896\n",
      "Epoch 341/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 707.5955 - val_loss: 668.6191\n",
      "Epoch 342/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 423.2389 - val_loss: 668.6216\n",
      "Epoch 343/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 2875.4434 - val_loss: 668.4711\n",
      "Epoch 344/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 412.0209 - val_loss: 668.5022\n",
      "Epoch 345/1000\n",
      "289/289 [==============================] - 0s 896us/step - loss: 411.7699 - val_loss: 668.4897\n",
      "Epoch 346/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 411.6802 - val_loss: 668.5453\n",
      "Epoch 347/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 411.8660 - val_loss: 668.4410\n",
      "Epoch 348/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 412.7448 - val_loss: 668.5131\n",
      "Epoch 349/1000\n",
      "289/289 [==============================] - 0s 959us/step - loss: 411.5434 - val_loss: 668.5525\n",
      "Epoch 350/1000\n",
      "289/289 [==============================] - 0s 941us/step - loss: 415.4937 - val_loss: 668.5210\n",
      "Epoch 351/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 441.8444 - val_loss: 668.5751\n",
      "Epoch 352/1000\n",
      "289/289 [==============================] - 0s 936us/step - loss: 411.4121 - val_loss: 668.6150\n",
      "Epoch 353/1000\n",
      "289/289 [==============================] - 0s 940us/step - loss: 412.5093 - val_loss: 668.5284\n",
      "Epoch 354/1000\n",
      "289/289 [==============================] - 0s 951us/step - loss: 419.6378 - val_loss: 668.5150\n",
      "Epoch 355/1000\n",
      "289/289 [==============================] - 0s 897us/step - loss: 489.7255 - val_loss: 668.3659\n",
      "Epoch 356/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 523.9112 - val_loss: 668.3073\n",
      "Epoch 357/1000\n",
      "289/289 [==============================] - 0s 965us/step - loss: 443.1676 - val_loss: 668.3203\n",
      "Epoch 358/1000\n",
      "289/289 [==============================] - 0s 938us/step - loss: 411.6739 - val_loss: 668.3260\n",
      "Epoch 359/1000\n",
      "289/289 [==============================] - 0s 914us/step - loss: 411.4355 - val_loss: 668.2776\n",
      "Epoch 360/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 720.5768 - val_loss: 668.2523\n",
      "Epoch 361/1000\n",
      "289/289 [==============================] - 0s 872us/step - loss: 411.7211 - val_loss: 668.2722\n",
      "Epoch 362/1000\n",
      "289/289 [==============================] - 0s 943us/step - loss: 415.2918 - val_loss: 668.2458\n",
      "Epoch 363/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 417.3955 - val_loss: 668.1810\n",
      "Epoch 364/1000\n",
      "289/289 [==============================] - 0s 995us/step - loss: 415.9942 - val_loss: 668.1937\n",
      "Epoch 365/1000\n",
      "289/289 [==============================] - 0s 993us/step - loss: 411.8944 - val_loss: 668.2283\n",
      "Epoch 366/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 470.2666 - val_loss: 668.1157\n",
      "Epoch 367/1000\n",
      "289/289 [==============================] - 0s 927us/step - loss: 422.8858 - val_loss: 668.0534\n",
      "Epoch 368/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6987 - val_loss: 668.1439\n",
      "Epoch 369/1000\n",
      "289/289 [==============================] - 0s 962us/step - loss: 415.1888 - val_loss: 668.1471\n",
      "Epoch 370/1000\n",
      "289/289 [==============================] - 0s 950us/step - loss: 910.0265 - val_loss: 668.2025\n",
      "Epoch 371/1000\n",
      "289/289 [==============================] - 0s 894us/step - loss: 411.4392 - val_loss: 668.2621\n",
      "Epoch 372/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 412.4223 - val_loss: 668.1891\n",
      "Epoch 373/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 921.1254 - val_loss: 668.0605\n",
      "Epoch 374/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 411.6292 - val_loss: 668.0307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/1000\n",
      "289/289 [==============================] - 0s 890us/step - loss: 544.3140 - val_loss: 668.1556\n",
      "Epoch 376/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 5549.1670 - val_loss: 668.0028\n",
      "Epoch 377/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 516.2723 - val_loss: 668.2044\n",
      "Epoch 378/1000\n",
      "289/289 [==============================] - 0s 981us/step - loss: 411.6552 - val_loss: 668.2450\n",
      "Epoch 379/1000\n",
      "289/289 [==============================] - 0s 946us/step - loss: 411.7033 - val_loss: 668.2892\n",
      "Epoch 380/1000\n",
      "289/289 [==============================] - 0s 905us/step - loss: 411.5820 - val_loss: 668.3617\n",
      "Epoch 381/1000\n",
      "289/289 [==============================] - 0s 975us/step - loss: 411.3758 - val_loss: 668.4288\n",
      "Epoch 382/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 2165.8911 - val_loss: 668.2906\n",
      "Epoch 383/1000\n",
      "289/289 [==============================] - 0s 968us/step - loss: 414.8547 - val_loss: 668.2818\n",
      "Epoch 384/1000\n",
      "289/289 [==============================] - 0s 875us/step - loss: 411.8652 - val_loss: 668.3656\n",
      "Epoch 385/1000\n",
      "289/289 [==============================] - 0s 896us/step - loss: 411.5990 - val_loss: 668.4493\n",
      "Epoch 386/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 413.2603 - val_loss: 668.5468\n",
      "Epoch 387/1000\n",
      "289/289 [==============================] - 0s 912us/step - loss: 411.8928 - val_loss: 668.5797\n",
      "Epoch 388/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7314 - val_loss: 668.6499\n",
      "Epoch 389/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.3942 - val_loss: 668.6981\n",
      "Epoch 390/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 417.5540 - val_loss: 668.6458\n",
      "Epoch 391/1000\n",
      "289/289 [==============================] - 0s 917us/step - loss: 411.7097 - val_loss: 668.7924\n",
      "Epoch 392/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.9627 - val_loss: 668.8375\n",
      "Epoch 393/1000\n",
      "289/289 [==============================] - 0s 980us/step - loss: 691.3786 - val_loss: 668.8257\n",
      "Epoch 394/1000\n",
      "289/289 [==============================] - 0s 973us/step - loss: 411.8473 - val_loss: 668.8321\n",
      "Epoch 395/1000\n",
      "289/289 [==============================] - 0s 951us/step - loss: 412.1104 - val_loss: 668.8414\n",
      "Epoch 396/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7531 - val_loss: 668.8871\n",
      "Epoch 397/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6008 - val_loss: 668.8714\n",
      "Epoch 398/1000\n",
      "289/289 [==============================] - 0s 912us/step - loss: 411.4587 - val_loss: 668.8830\n",
      "Epoch 399/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 413.0964 - val_loss: 668.8661\n",
      "Epoch 400/1000\n",
      "289/289 [==============================] - 0s 969us/step - loss: 788.0668 - val_loss: 668.8096\n",
      "Epoch 401/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 435.3656 - val_loss: 668.7329\n",
      "Epoch 402/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 413.2532 - val_loss: 668.6622\n",
      "Epoch 403/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7534 - val_loss: 668.7812\n",
      "Epoch 404/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7469 - val_loss: 668.9024\n",
      "Epoch 405/1000\n",
      "289/289 [==============================] - 0s 946us/step - loss: 411.3217 - val_loss: 668.9388\n",
      "Epoch 406/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 414.4176 - val_loss: 668.8506\n",
      "Epoch 407/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.3985 - val_loss: 668.8693\n",
      "Epoch 408/1000\n",
      "289/289 [==============================] - 0s 878us/step - loss: 411.8167 - val_loss: 668.8885\n",
      "Epoch 409/1000\n",
      "289/289 [==============================] - 0s 984us/step - loss: 411.5037 - val_loss: 668.8857\n",
      "Epoch 410/1000\n",
      "289/289 [==============================] - 0s 997us/step - loss: 411.6398 - val_loss: 668.9110\n",
      "Epoch 411/1000\n",
      "289/289 [==============================] - 0s 992us/step - loss: 411.6496 - val_loss: 668.8594\n",
      "Epoch 412/1000\n",
      "289/289 [==============================] - 0s 987us/step - loss: 411.5815 - val_loss: 668.8806\n",
      "Epoch 413/1000\n",
      "289/289 [==============================] - 0s 966us/step - loss: 411.5691 - val_loss: 668.8528\n",
      "Epoch 414/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7200 - val_loss: 668.8397\n",
      "Epoch 415/1000\n",
      "289/289 [==============================] - 0s 941us/step - loss: 644.9474 - val_loss: 668.5830\n",
      "Epoch 416/1000\n",
      "289/289 [==============================] - 0s 898us/step - loss: 411.3973 - val_loss: 668.5394\n",
      "Epoch 417/1000\n",
      "289/289 [==============================] - 0s 877us/step - loss: 411.7296 - val_loss: 668.4791\n",
      "Epoch 418/1000\n",
      "289/289 [==============================] - 0s 988us/step - loss: 411.6799 - val_loss: 668.4452\n",
      "Epoch 419/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5517 - val_loss: 668.3813\n",
      "Epoch 420/1000\n",
      "289/289 [==============================] - 0s 953us/step - loss: 411.9406 - val_loss: 668.4305\n",
      "Epoch 421/1000\n",
      "289/289 [==============================] - 0s 948us/step - loss: 411.5804 - val_loss: 668.4295\n",
      "Epoch 422/1000\n",
      "289/289 [==============================] - 0s 960us/step - loss: 411.3711 - val_loss: 668.3724\n",
      "Epoch 423/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 452.1450 - val_loss: 668.2435\n",
      "Epoch 424/1000\n",
      "289/289 [==============================] - 0s 946us/step - loss: 424.7227 - val_loss: 668.1597\n",
      "Epoch 425/1000\n",
      "289/289 [==============================] - 0s 947us/step - loss: 411.5840 - val_loss: 668.1683\n",
      "Epoch 426/1000\n",
      "289/289 [==============================] - 0s 888us/step - loss: 441.1964 - val_loss: 668.1454\n",
      "Epoch 427/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 411.6091 - val_loss: 668.1420\n",
      "Epoch 428/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8685 - val_loss: 668.1037\n",
      "Epoch 429/1000\n",
      "289/289 [==============================] - 0s 899us/step - loss: 411.7101 - val_loss: 668.0628\n",
      "Epoch 430/1000\n",
      "289/289 [==============================] - 0s 932us/step - loss: 7238.3779 - val_loss: 668.1879\n",
      "Epoch 431/1000\n",
      "289/289 [==============================] - 0s 896us/step - loss: 411.7997 - val_loss: 668.2347\n",
      "Epoch 432/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5233 - val_loss: 668.3297\n",
      "Epoch 433/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6963 - val_loss: 668.3538\n",
      "Epoch 434/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 519.7977 - val_loss: 668.5519\n",
      "Epoch 435/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6855 - val_loss: 668.5519\n",
      "Epoch 436/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1320.6545 - val_loss: 668.5852\n",
      "Epoch 437/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7591 - val_loss: 668.6129\n",
      "Epoch 438/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 411.3717 - val_loss: 668.6071\n",
      "Epoch 439/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8628 - val_loss: 668.6296\n",
      "Epoch 440/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 426.1265 - val_loss: 668.6360\n",
      "Epoch 441/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.4255 - val_loss: 668.6310\n",
      "Epoch 442/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 551.4525 - val_loss: 668.6109\n",
      "Epoch 443/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5068 - val_loss: 668.6405\n",
      "Epoch 444/1000\n",
      "289/289 [==============================] - 0s 870us/step - loss: 411.7764 - val_loss: 668.6277\n",
      "Epoch 445/1000\n",
      "289/289 [==============================] - 0s 958us/step - loss: 767.4829 - val_loss: 668.5770\n",
      "Epoch 446/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 411.8012 - val_loss: 668.5865\n",
      "Epoch 447/1000\n",
      "289/289 [==============================] - 0s 950us/step - loss: 411.2701 - val_loss: 668.5045\n",
      "Epoch 448/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5958 - val_loss: 668.4750\n",
      "Epoch 449/1000\n",
      "289/289 [==============================] - 0s 919us/step - loss: 411.7807 - val_loss: 668.4554\n",
      "Epoch 450/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8664 - val_loss: 668.4664\n",
      "Epoch 451/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7216 - val_loss: 668.4052\n",
      "Epoch 452/1000\n",
      "289/289 [==============================] - 0s 986us/step - loss: 412.4218 - val_loss: 668.2900\n",
      "Epoch 453/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6971 - val_loss: 668.2688\n",
      "Epoch 454/1000\n",
      "289/289 [==============================] - 0s 971us/step - loss: 459.4941 - val_loss: 668.2108\n",
      "Epoch 455/1000\n",
      "289/289 [==============================] - 0s 970us/step - loss: 1461.1821 - val_loss: 668.1588\n",
      "Epoch 456/1000\n",
      "289/289 [==============================] - 0s 917us/step - loss: 411.6878 - val_loss: 668.1424\n",
      "Epoch 457/1000\n",
      "289/289 [==============================] - 0s 974us/step - loss: 411.7689 - val_loss: 668.1232\n",
      "Epoch 458/1000\n",
      "289/289 [==============================] - 0s 988us/step - loss: 419.3241 - val_loss: 668.0963\n",
      "Epoch 459/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6513 - val_loss: 668.1448\n",
      "Epoch 460/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 2239.4570 - val_loss: 668.0577\n",
      "Epoch 461/1000\n",
      "289/289 [==============================] - 0s 918us/step - loss: 411.7100 - val_loss: 668.1451\n",
      "Epoch 462/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7099 - val_loss: 668.1987\n",
      "Epoch 463/1000\n",
      "289/289 [==============================] - 0s 978us/step - loss: 411.6435 - val_loss: 668.2598\n",
      "Epoch 464/1000\n",
      "289/289 [==============================] - 0s 943us/step - loss: 411.6095 - val_loss: 668.2711\n",
      "Epoch 465/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6831 - val_loss: 668.3829\n",
      "Epoch 466/1000\n",
      "289/289 [==============================] - 0s 984us/step - loss: 411.3335 - val_loss: 668.3365\n",
      "Epoch 467/1000\n",
      "289/289 [==============================] - 0s 989us/step - loss: 1061.9773 - val_loss: 668.3810\n",
      "Epoch 468/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.4528 - val_loss: 668.2815\n",
      "Epoch 469/1000\n",
      "289/289 [==============================] - 0s 912us/step - loss: 2239.1394 - val_loss: 668.3041\n",
      "Epoch 470/1000\n",
      "289/289 [==============================] - 0s 941us/step - loss: 411.7184 - val_loss: 668.2932\n",
      "Epoch 471/1000\n",
      "289/289 [==============================] - 0s 967us/step - loss: 562.1928 - val_loss: 668.2788\n",
      "Epoch 472/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7794 - val_loss: 668.3231\n",
      "Epoch 473/1000\n",
      "289/289 [==============================] - 0s 923us/step - loss: 419.6057 - val_loss: 668.4047\n",
      "Epoch 474/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6472 - val_loss: 668.4371\n",
      "Epoch 475/1000\n",
      "289/289 [==============================] - 0s 961us/step - loss: 411.6283 - val_loss: 668.4163\n",
      "Epoch 476/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 457.7041 - val_loss: 668.3571\n",
      "Epoch 477/1000\n",
      "289/289 [==============================] - 0s 954us/step - loss: 411.8262 - val_loss: 668.4195\n",
      "Epoch 478/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 482.2928 - val_loss: 668.4070\n",
      "Epoch 479/1000\n",
      "289/289 [==============================] - 0s 984us/step - loss: 411.8072 - val_loss: 668.4780\n",
      "Epoch 480/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6747 - val_loss: 668.5411\n",
      "Epoch 481/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 423.1878 - val_loss: 668.5718\n",
      "Epoch 482/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.4911 - val_loss: 668.6473\n",
      "Epoch 483/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.6302 - val_loss: 668.5924\n",
      "Epoch 484/1000\n",
      "289/289 [==============================] - 0s 918us/step - loss: 411.8465 - val_loss: 668.6106\n",
      "Epoch 485/1000\n",
      "289/289 [==============================] - 0s 928us/step - loss: 411.6628 - val_loss: 668.6765\n",
      "Epoch 486/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5971 - val_loss: 668.6907\n",
      "Epoch 487/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.8491 - val_loss: 668.7252\n",
      "Epoch 488/1000\n",
      "289/289 [==============================] - 0s 957us/step - loss: 411.5752 - val_loss: 668.7798\n",
      "Epoch 489/1000\n",
      "289/289 [==============================] - 0s 922us/step - loss: 638.6520 - val_loss: 668.8759\n",
      "Epoch 490/1000\n",
      "289/289 [==============================] - 0s 899us/step - loss: 411.8041 - val_loss: 668.8846\n",
      "Epoch 491/1000\n",
      "289/289 [==============================] - 0s 973us/step - loss: 613.1444 - val_loss: 668.8644\n",
      "Epoch 492/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7776 - val_loss: 668.8911\n",
      "Epoch 493/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5152 - val_loss: 668.8919\n",
      "Epoch 494/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7697 - val_loss: 668.8533\n",
      "Epoch 495/1000\n",
      "289/289 [==============================] - 0s 960us/step - loss: 411.7672 - val_loss: 668.8636\n",
      "Epoch 496/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5027 - val_loss: 668.8915\n",
      "Epoch 497/1000\n",
      "289/289 [==============================] - 0s 955us/step - loss: 620.5290 - val_loss: 668.8718\n",
      "Epoch 498/1000\n",
      "289/289 [==============================] - 0s 972us/step - loss: 411.4318 - val_loss: 668.8737\n",
      "Epoch 499/1000\n",
      "289/289 [==============================] - 0s 929us/step - loss: 411.8553 - val_loss: 668.8603\n",
      "Epoch 500/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.7225 - val_loss: 668.8947\n",
      "Epoch 501/1000\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 411.5599 - val_loss: 668.8890\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "model = ffnn(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69e619ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 676us/step\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "y_pred = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9f11535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2104, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676227ca",
   "metadata": {},
   "source": [
    "#### end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fe7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list, mape_list = [], []\n",
    "for i in range(len(latitudes)):\n",
    "    rmse, mape = cross_validation(final_df, i)\n",
    "    rmse_list.append(rmse)\n",
    "    mape_list.append(mape)\n",
    "    print(f'{device_ids[i]} successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rmse = np.mean(rmse_list)          \n",
    "mean_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65808b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mape = np.mean(mape_list)          \n",
    "mean_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a90d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
