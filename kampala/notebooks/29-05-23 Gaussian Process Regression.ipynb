{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from math import sqrt\n",
    "sys.path.append('../..')\n",
    "from modules import utils\n",
    "import gpflow\n",
    "from gpflow import set_trainable\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "# random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "# gpflow.config.set_default_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kampala_devices = pd.read_csv('../data/kampala_devices.csv', usecols=['lat', 'long', 'id'])\n",
    "kampala_devices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kampala_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kampala_df = pd.read_csv('../data/kampala_data_with_met.csv', parse_dates=['timestamp'])\n",
    "kampala_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes = kampala_df['latitude'].unique()\n",
    "longitudes = kampala_df['longitude'].unique()\n",
    "device_ids = kampala_df['device_number'].unique()\n",
    "len(latitudes), len(longitudes), len(device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "cols = ['timestamp', 'latitude', 'longitude', 'pm2_5_calibrated_value', 'temperature', 'humidity', 'wind_speed', 'wind_direction']\n",
    "# for i, device_id in enumerate(device_ids):\n",
    "for i, device_id in kampala_devices.id.iteritems():\n",
    "    device_df = utils.get_device_data(kampala_df, device_id, cols)\n",
    "    processed_df = utils.preprocessing(device_df)\n",
    "#     print(f'{device_id}: {len(processed_df)}')\n",
    "    final_df = pd.concat([final_df, processed_df])\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df), len(final_df.latitude.unique()), len(final_df.longitude.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be929aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(final_df, idx, kernel_variance, lengthscales, likelihood_variance, trainable_kernel, \n",
    "                     trainable_variance, trainable_lengthscales):\n",
    "    device_indices = final_df[final_df.latitude==latitudes[idx]].index\n",
    "    device_df = kampala_df[kampala_df.device_number == device_ids[idx]]\n",
    "#     assert(len(device_indices) == len(device_df)-device_df.pm2_5_calibrated_value.isna().sum())\n",
    "    \n",
    "    test_df = final_df.loc[device_indices]\n",
    "#     assert(len(test_df.longitude.unique()) == 1)\n",
    "    \n",
    "    train_df = pd.concat([final_df, test_df]).drop_duplicates(keep=False)\n",
    "#     assert(len(train_df.longitude.unique()) == len(longitudes)-1)\n",
    "#     assert len(final_df) == len(test_df) + len(train_df)\n",
    "    \n",
    "    X_train = train_df.iloc[:, 0:-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train).reshape(-1, 1)\n",
    "    if X_train.shape[0] > 39999:\n",
    "        X_train = X_train[::6, :]\n",
    "        y_train = y_train[::6, :]\n",
    "#     print('printing x_train')\n",
    "#     print(f'X_train shape:{X_train.shape}')\n",
    "    \n",
    "    X_test = test_df.iloc[:, 0:-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test).reshape(-1, 1)\n",
    "    #to delete\n",
    "    #X_train, y_train, X_test, y_test = X_train[:100, :], y_train[:100, :], X_test[:100, :], y_test[:100, :]\n",
    "    \n",
    "    if lengthscales == 'train_shape':\n",
    "        lengthscales = np.ones(X_train.shape[1])\n",
    "    \n",
    "    if (lengthscales is None) & (kernel_variance is None):\n",
    "        k = gpflow.kernels.RBF() + gpflow.kernels.Bias()\n",
    "    elif lengthscales is None:\n",
    "        k = gpflow.kernels.RBF(variance=kernel_variance) + gpflow.kernels.Bias()\n",
    "    elif kernel_variance is None:\n",
    "        k = gpflow.kernels.RBF(lengthscales=lengthscales) + gpflow.kernels.Bias()\n",
    "    else:\n",
    "        k = gpflow.kernels.RBF(lengthscales=lengthscales, variance=kernel_variance) + gpflow.kernels.Bias()\n",
    "#     print('Training model .....................')    \n",
    "    m = gpflow.models.GPR(data=(X_train, y_train), kernel=k, mean_function=None)\n",
    "    if likelihood_variance is None:\n",
    "        pass\n",
    "    else:\n",
    "        m.likelihood.variance.assign(likelihood_variance)\n",
    "    set_trainable(m.kernel.kernels[0].variance, trainable_kernel)\n",
    "    set_trainable(m.likelihood.variance, trainable_variance)\n",
    "    set_trainable(m.kernel.kernels[0].lengthscales, trainable_lengthscales)\n",
    "    \n",
    "    #optimization\n",
    "#     print('Optimizing model ...........................')\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    def objective_closure():\n",
    "        return - m.log_marginal_likelihood()\n",
    "    \n",
    "    opt_logs = opt.minimize(objective_closure,\n",
    "                            m.trainable_variables,\n",
    "                            options=dict(maxiter=100))\n",
    "\n",
    "    #prediction\n",
    "    mean, var = m.predict_f(X_test)\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(y_test, mean.numpy()))\n",
    "    mape = mean_absolute_percentage_error(y_test, mean.numpy())\n",
    "    return rmse, mape\n",
    "    \n",
    "#     return mean.numpy(), var.numpy(), Xtest, Ytest, round(rmse, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928bfce",
   "metadata": {},
   "source": [
    "#### The real work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengthscales = [[0.08, 0.08, 1], None, 'train_shape']\n",
    "# lengthscales = [[0.08, 0.08, 1]]\n",
    "lengthscale = None\n",
    "# likelihood_variances = [400, 625, None]\n",
    "likelihood_variance = 400\n",
    "# kernel_variances = [625, 400, None]\n",
    "kernel_variance  = 625\n",
    "# trainable_kernels = [True, False]\n",
    "trainable_kernel = True\n",
    "# trainable_variances = [True, False]\n",
    "trainable_variance = True\n",
    "# trainable_lengthscales = [False, True]\n",
    "trainable_lengthscale = True #only changed this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146de88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "results_df = pd.DataFrame()\n",
    "# for lengthscale in lengthscales:\n",
    "#     for likelihood_variance in likelihood_variances:\n",
    "#         for kernel_variance in kernel_variances:\n",
    "#             for trainable_kernel in trainable_kernels:\n",
    "#                 for trainable_variance in trainable_variances:\n",
    "#                     for trainable_lengthscale in trainable_lengthscales:\n",
    "#                         count+=1\n",
    "rmse_list = []\n",
    "mape_list = []\n",
    "#                         print(f'EXPERIMENT {count}')\n",
    "for i in range(len(latitudes)):\n",
    "    try:\n",
    "        rmse, mape = cross_validation(final_df, i, kernel_variance, lengthscale, likelihood_variance, \n",
    "                                trainable_kernel, trainable_variance, trainable_lengthscale)\n",
    "        rmse_list.append(rmse)\n",
    "        mape_list.append(mape)\n",
    "        print(f'{device_ids[i]} successful')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'{device_ids[i]} failed')\n",
    "        break\n",
    "\n",
    "mean_rmse = np.mean(rmse_list)\n",
    "mean_mape = np.mean(mape_list)\n",
    "results_dict= {'lengthscale':lengthscale, 'likelihood_variance':likelihood_variance, \n",
    "               'kernel_variance':kernel_variance, 'trainable_kernel':trainable_kernel, \n",
    "               'trainable_variance':trainable_variance, 'trainable_lengthscale':trainable_lengthscale, \n",
    "               'avg_rmse':mean_rmse, 'rmse_list':rmse_list, 'avg_mape':mean_mape, 'mape_list':mape_list}\n",
    "print(results_dict)\n",
    "results_df = results_df.append(results_dict, ignore_index=True)\n",
    "results_df.to_csv(f'../results/basic_results_with_met.csv', index=False)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac260049",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
